{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from delta.tables import DeltaTable\n",
        "from pyspark.sql.window import Window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### *Incremnetal Load Date*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "source_schema = 'silver_sales'\n",
        "\n",
        "target_schema = 'gold_sales'\n",
        "\n",
        "backdate = ''\n",
        "\n",
        "cdc_col = 'ModifiedDate'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "if spark.catalog.tableExists(f\"{target_schema}.FactOrder\"):\n",
        "    if len(backdate) == 0:\n",
        "        last_load_date = spark.sql(f\"\"\"\n",
        "                                SELECT MAX({cdc_col}) FROM {target_schema}.FactOrder\n",
        "                                \"\"\").collect()[0][0]\n",
        "    else:\n",
        "        last_load_date = backdate\n",
        "else:\n",
        "    last_load_date = '1900-01-01 00:00:00'\n",
        "\n",
        "last_load_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_src = spark.sql(f\"\"\"\n",
        "                    SELECT * FROM {source_schema}.salesorderheader \n",
        "                    WHERE {cdc_col} > '{last_load_date}'\n",
        "                    \"\"\")\n",
        "\n",
        "df_src.createOrReplaceTempView(\"src\")\n",
        "\n",
        "df_src.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_fact_src = spark.sql(\"\"\"\n",
        "                Select \n",
        "                    sh.SalesOrderID,\n",
        "                    dr.DimResellerKey,\n",
        "                    drc.DimCustomerKey,\n",
        "                    dsp.DimSalesPersonKey,\n",
        "                    sh.RevisionNumber, sh.OrderDate, sh.DueDate, sh.ShipDate, sh.Status, sh.OnlineOrderFlag, sh.SalesOrderNumber,\n",
        "                    sh.PurchaseOrderNumber, sh.AccountNumber, sh.TerritoryID, sh.BillToAddressID, sh.ShipToAddressID, sh.ShipMethodID,\n",
        "                    sh.CreditCardID, sh.Subtotal, sh.TaxAmt, sh.Freight, sh.TotalDue, sh.ModifiedDate\n",
        "                FROM\n",
        "                src sh\n",
        "                LEFT JOIN gold_sales.dimreseller dr\n",
        "                    ON sh.CustomerID = dr.CustomerID\n",
        "                    AND sh.OrderDate >= dr.StartDate\n",
        "                    AND sh.OrderDate < dr.EndDate\n",
        "                LEFT JOIN gold_sales.dimretailcustomer drc\n",
        "                    ON sh.CustomerID = drc.CustomerID\n",
        "                    AND sh.OrderDate >= drc.StartDate\n",
        "                    AND sh.OrderDate < drc.EndDate\n",
        "                LEFT JOIN gold_sales.dimsalesperson dsp\n",
        "                    ON sh.SalesPersonID = dsp.SalesPersonID\n",
        "                    AND sh.OrderDate >= dsp.StartDate\n",
        "                    AND sh.OrderDate < dsp.EndDate\n",
        "                \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "if spark.catalog.tableExists(f\"{target_schema}.FactOrder\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, f\"{target_schema}.FactOrder\")\n",
        "    dlt_obj.alias(\"trg\").merge(df_fact_src.alias(\"src\"), \"trg.SalesOrderID = src.SalesOrderID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=f\"src.{cdc_col} > trg.{cdc_col}\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "else:\n",
        "    df_fact_src.write.format('delta')\\\n",
        "                .mode('append')\\\n",
        "                .option('path', 'abfss://gold@dlcontoso.dfs.core.windows.net/sales/FactOrder')\\\n",
        "                .saveAsTable('gold_sales.FactOrder')"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}