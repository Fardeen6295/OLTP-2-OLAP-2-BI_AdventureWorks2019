{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from delta.tables import DeltaTable\n",
        "from pyspark.sql.window import Window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### *Incremnetal Load Date*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {},
      "source": [
        "source_schema = 'silver_sales'\n",
        "\n",
        "target_schema = 'gold_sales'\n",
        "\n",
        "backdate = ''\n",
        "\n",
        "cdc_col = 'ModifiedDate'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {},
      "source": [
        "if spark.catalog.tableExists(f\"{target_schema}.FactOrderDetail\"):\n",
        "    if len(backdate) == 0:\n",
        "        last_load_date = spark.sql(f\"\"\"\n",
        "                                SELECT MAX({cdc_col}) FROM {target_schema}.FactOrderDetail\n",
        "                                \"\"\").collect()[0][0]\n",
        "    else:\n",
        "        last_load_date = backdate\n",
        "else:\n",
        "    last_load_date = '1900-01-01 00:00:00'\n",
        "\n",
        "last_load_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_src = spark.sql(f\"\"\"\n",
        "                    SELECT * FROM {source_schema}.salesorderdetail \n",
        "                    WHERE {cdc_col} > '{last_load_date}'\n",
        "                    \"\"\")\n",
        "\n",
        "df_src.createOrReplaceTempView(\"src\")\n",
        "\n",
        "df_src.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_fact_src = spark.sql(\"\"\"\n",
        "                Select \n",
        "                    sh.SalesOrderID,\n",
        "                    sh.SalesOrderDetailID,\n",
        "                    sh.CarrierTrackingNumber,\n",
        "                    sh.OrderQty,\n",
        "                    dp.DimProductKey,\n",
        "                    sh.SpecialOfferID, sh.UnitPrice, sh.UnitPriceDiscount, sh.LineTotal, sh.ModifiedDate\n",
        "                FROM\n",
        "                src sh\n",
        "                LEFT JOIN gold_sales.dimproduct dp\n",
        "                    ON sh.ProductID = dp.ProductID\n",
        "                    AND sh.ModifiedDate >= dp.StartDate\n",
        "                    AND sh.ModifiedDate < dp.EndDate\n",
        "                \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {},
      "source": [
        "if spark.catalog.tableExists(f\"{target_schema}.FactOrderDetail\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, f\"{target_schema}.FactOrderDetail\")\n",
        "    dlt_obj.alias(\"trg\").merge(df_fact_src.alias(\"src\"), \"trg.SalesOrderID = src.SalesOrderID AND trg.SalesOrderDetailID = src.SalesOrderDetailID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=f\"src.{cdc_col} > trg.{cdc_col}\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "else:\n",
        "    df_fact_src.write.format('delta')\\\n",
        "                .mode('append')\\\n",
        "                .option('path', 'abfss://gold@dlcontoso.dfs.core.windows.net/sales/FactOrderDetail')\\\n",
        "                .saveAsTable('gold_sales.FactOrderDetail')"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}