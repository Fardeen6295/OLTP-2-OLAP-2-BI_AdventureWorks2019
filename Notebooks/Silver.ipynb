{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# *Silver Sales Tables Transformations*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from delta.tables import DeltaTable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Sales Header"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format(\"parquet\")\\\n",
        "            .load(\"abfss://bronze@dlcontoso.dfs.core.windows.net/sales/SalesOrderHeader\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.drop(col(\"CreditCardApprovalCode\"), col(\"CurrencyRateID\"), col(\"Comment\"), col(\"rowguid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "if spark.catalog.tableExists(\"Silver_Sales.SalesOrderHeader\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"Silver_Sales.SalesOrderHeader\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.SalesOrderID = src.SalesOrderID\")\\\n",
        "            .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "            .whenNotMatchedInsertAll()\\\n",
        "            .execute()\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "            .mode(\"append\")\\\n",
        "            .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/sales/SalesOrderHeader\")\\\n",
        "            .saveAsTable(\"Silver_Sales.SalesOrderHeader\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Sales Details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format(\"parquet\")\\\n",
        "            .load(\"abfss://bronze@dlcontoso.dfs.core.windows.net/sales/SalesOrderDetail\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.drop(col(\"rowguid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "if spark.catalog.tableExists(\"silver_sales.SalesOrderDetail\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.SalesOrderDetail\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.SalesOrderID = src.SalesOrderID AND trg.SalesOrderDetailID = src.SalesOrderDetailID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "            .mode(\"append\")\\\n",
        "            .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/sales/SalesOrderDetail\")\\\n",
        "            .saveAsTable(\"silver_sales.SalesOrderDetail\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Customers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/sales/Customer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_reseller = df.filter(col(\"StoreID\").isNotNull())\n",
        "df_reseller = df_reseller.drop(col(\"rowguid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "df_retailCust = df.filter(col(\"StoreID\").isNull())\n",
        "df_retailCust = df_retailCust.drop(col(\"rowguid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "if spark.catalog.tableExists(\"silver_sales.Reseller\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.Reseller\")\n",
        "    dlt_obj.alias(\"trg\").merge(df_reseller.alias(\"src\"), \"trg.CustomerID = src.CustomerID\")\\\n",
        "                .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                .whenNotMatchedInsertAll()\\\n",
        "                .execute()\n",
        "else:\n",
        "    df_reseller.write.format(\"delta\")\\\n",
        "            .mode(\"append\")\\\n",
        "            .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/sales/Reseller\")\\\n",
        "            .saveAsTable(\"silver_sales.Reseller\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "if spark.catalog.tableExists(\"silver_sales.RetailCustomer\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.RetailCustomer\")\n",
        "    dlt_obj.alias(\"trg\").merge(df_retailCust.alias(\"src\"), \"trg.CustomerID = src.CustomerID\")\\\n",
        "                .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                .whenNotMatchedInsertAll()\\\n",
        "                .execute()\n",
        "else:\n",
        "    df_reseller.write.format(\"delta\")\\\n",
        "            .mode(\"append\")\\\n",
        "            .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/sales/RetailCustomer\")\\\n",
        "            .saveAsTable(\"silver_sales.RetailCustomer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### BusinessEntityAddress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/person/BusinessEntityAddress')\n",
        "\n",
        "df = df.drop(col(\"rowguid\"))\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.BusinessEntityAddress\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.BusinessEntityAddress\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.BusinessEntityID = src.BusinessEntityID\")\\\n",
        "                .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                .whenNotMatchedInsertAll()\\\n",
        "                .execute()\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/person/BusinessEntityAddress\")\\\n",
        "        .saveAsTable(\"silver_sales.BusinessEntityAddress\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/person/vw_Address_Clean')\n",
        "\n",
        "df = df.drop(col(\"rowguid\"), col(\"AddressLine2\"), col(\"SpatialLocation\"))\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.Address\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.Address\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.AddressID = src.AddressID\")\\\n",
        "                .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                .whenNotMatchedInsertAll()\\\n",
        "                .execute()\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/person/Address\")\\\n",
        "        .saveAsTable(\"silver_sales.Address\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/person/AddressType')\n",
        "\n",
        "df = df.drop(col(\"rowguid\"))\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.AddressType\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.AddressType\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.AddressTypeID = src.AddressTypeID\")\\\n",
        "                .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                .whenNotMatchedInsertAll()\\\n",
        "                .execute()\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/person/AddressType\")\\\n",
        "        .saveAsTable(\"silver_sales.AddressType\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/person/StateProvince')\n",
        "\n",
        "df = df.drop(col(\"rowguid\"))\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.StateProvince\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.StateProvince\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.StateProvinceID = src.StateProvinceID\")\\\n",
        "                .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                .whenNotMatchedInsertAll()\\\n",
        "                .execute()\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/person/StateProvince\")\\\n",
        "        .saveAsTable(\"silver_sales.StateProvince\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/person/CountryRegion')\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.CountryRegion\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.CountryRegion\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.CountryRegionCode = src.CountryRegionCode\")\\\n",
        "                .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                .whenNotMatchedInsertAll()\\\n",
        "                .execute()\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/person/CountryRegion\")\\\n",
        "        .saveAsTable(\"silver_sales.CountryRegion\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/sales/SalesTerritory')\n",
        "\n",
        "df = df.drop(col(\"SalesYTD\"), col(\"SalesLastYear\"), col(\"CostYTD\"), col(\"CostLastYear\"), col(\"rowguid\"))\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.SalesTerritory\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.SalesTerritory\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.TerritoryID = src.TerritoryID\")\\\n",
        "                .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                .whenNotMatchedInsertAll()\\\n",
        "                .execute()\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/sales/SalesTerritory\")\\\n",
        "        .saveAsTable(\"silver_sales.SalesTerritory\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/person/EmailAddress')\n",
        "\n",
        "df = df.drop(col(\"rowguid\"))\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.EmailAddress\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.EmailAddress\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.BusinessEntityID = src.BusinessEntityID AND trg.EmailAddressID = src.EmailAddressID\")\\\n",
        "                .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                .whenNotMatchedInsertAll()\\\n",
        "                .execute()\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/person/EmailAddress\")\\\n",
        "        .saveAsTable(\"silver_sales.EmailAddress\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/sales/Store')\n",
        "\n",
        "df = df.drop(col(\"rowguid\"), col(\"Demographics\"))\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.Store\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.Store\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.BusinessEntityID = src.BusinessEntityID\")\\\n",
        "                .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                .whenNotMatchedInsertAll()\\\n",
        "                .execute()\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "        .mode(\"append\")\\\n",
        "        .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/sales/Store\")\\\n",
        "        .saveAsTable(\"silver_sales.Store\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### SalesReason"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format(\"parquet\")\\\n",
        "            .load(\"abfss://bronze@dlcontoso.dfs.core.windows.net/sales/SalesOrderHeaderSalesReason\")\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.OrderSalesReason\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.OrderSalesReason\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.SalesOrderID = src.SalesOrderID AND trg.SalesReasonID = src.SalesReasonID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "            .mode(\"append\")\\\n",
        "            .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/sales/OrderSalesReason\")\\\n",
        "            .saveAsTable(\"silver_sales.OrderSalesReason\")            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format(\"parquet\")\\\n",
        "            .load(\"abfss://bronze@dlcontoso.dfs.core.windows.net/sales/SalesReason\")\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.SalesReason\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.SalesReason\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.SalesReasonID = src.SalesReasonID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "            .mode(\"append\")\\\n",
        "            .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/sales/SalesReason\")\\\n",
        "            .saveAsTable(\"silver_sales.SalesReason\")            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Special Offer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format(\"parquet\")\\\n",
        "            .load(\"abfss://bronze@dlcontoso.dfs.core.windows.net/sales/SpecialOffer\")\n",
        "\n",
        "df = df.drop(col(\"rowguid\"))\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.SpecialOffer\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.SpecialOffer\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.SpecialOfferID = src.SpecialOfferID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "            .mode(\"append\")\\\n",
        "            .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/sales/SpecialOffer\")\\\n",
        "            .saveAsTable(\"silver_sales.SpecialOffer\")            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Perosn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format(\"parquet\")\\\n",
        "            .load(\"abfss://bronze@dlcontoso.dfs.core.windows.net/person/Person\")\n",
        "\n",
        "df = df.drop(col(\"rowguid\"), col(\"AdditionalContactInfo\"), col(\"Demographics\"), col(\"Suffix\"))\n",
        "\n",
        "\n",
        "df = df.withColumn(\"PersonType\", when(col(\"PersonType\") == \"IN\", \"Individual Retail Customer\")\\\n",
        "                            .when(col(\"PersonType\") == \"SC\", \"Store Contact\")\\\n",
        "                            .when(col(\"PersonType\") == \"EM\", \"Employee\")\\\n",
        "                            .when(col(\"PersonType\") == \"SP\", \"Sales Person\")\\\n",
        "                            .when(col(\"PersonType\") == \"VC\", \"Vendor Contact\")\\\n",
        "                            .when(col(\"PersonType\") == \"GC\", \"General Contact\"))\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.Person\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.Person\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.BusinessEntityID = src.BusinessEntityID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "            .mode(\"append\")\\\n",
        "            .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/person/Person\")\\\n",
        "            .saveAsTable(\"silver_sales.Person\")            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format(\"parquet\")\\\n",
        "            .load(\"abfss://bronze@dlcontoso.dfs.core.windows.net/person/PersonPhone\")\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.PersonPhone\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.PersonPhone\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.BusinessEntityID = src.BusinessEntityID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "            .mode(\"append\")\\\n",
        "            .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/person/PersonPhone\")\\\n",
        "            .saveAsTable(\"silver_sales.PersonPhone\")            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format(\"parquet\")\\\n",
        "            .load(\"abfss://bronze@dlcontoso.dfs.core.windows.net/person/PhoneNumberType\")\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.PhoneNumberType\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.PhoneNumberType\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.PhoneNumberTypeID = src.PhoneNumberTypeID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "            .mode(\"append\")\\\n",
        "            .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/person/PhoneNumberType\")\\\n",
        "            .saveAsTable(\"silver_sales.PhoneNumberType\")            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format(\"parquet\")\\\n",
        "            .load(\"abfss://bronze@dlcontoso.dfs.core.windows.net/sales/SalesPerson\")\n",
        "\n",
        "df = df.drop(col(\"SalesYTD\"), col(\"SalesLastYear\"), col(\"rowguid\"))\n",
        "\n",
        "if spark.catalog.tableExists(\"silver_sales.SalesPerson\"):\n",
        "    dlt_obj = DeltaTable.forName(spark, \"silver_sales.SalesPerson\")\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.BusinessEntityID = src.BusinessEntityID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "\n",
        "else:\n",
        "    df.write.format(\"delta\")\\\n",
        "            .mode(\"append\")\\\n",
        "            .option(\"path\", \"abfss://silver@dlcontoso.dfs.core.windows.net/sales/SalesPerson\")\\\n",
        "            .saveAsTable(\"silver_sales.SalesPerson\")            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/production/Product')\n",
        "\n",
        "df = df.drop(col(\"rowguid\"))\n",
        "\n",
        "df = df.withColumn(\"ProductLine\", trim(col(\"ProductLine\")))\\\n",
        "        .withColumn(\"Class\", trim(col(\"Class\")))\\\n",
        "        .withColumn(\"Style\", trim(col(\"Style\")))\n",
        "\n",
        "df = df.withColumn(\"ProductLine\", when(col(\"ProductLine\") == \"R\", \"Road\")\\\n",
        "                                    .when(col(\"ProductLine\") == \"M\", \"Mountain\")\\\n",
        "                                    .when(col(\"ProductLine\") == \"T\", \"Touring\")\\\n",
        "                                    .when(col(\"ProductLine\") == \"S\", \"Standard\")\\\n",
        "                                    .otherwise(\"General\")\n",
        "                )\n",
        "\n",
        "df = df.withColumn(\"Class\", when(col(\"Class\") == \"H\", \"High\")\\\n",
        "                            .when(col(\"Class\") == \"M\", \"Medium\")\\\n",
        "                            .when(col(\"Class\") == \"L\", \"Low\")\\\n",
        "                            .otherwise(\"Other\")\n",
        "                )\n",
        "\n",
        "df = df.withColumn(\"Style\", when(col(\"Style\") == \"M\", \"Men\")\\\n",
        "                            .when(col(\"Style\") == \"W\", \"Women\")\\\n",
        "                            .when(col(\"Style\") == \"U\", \"Universal\")\\\n",
        "                            .otherwise(\"No-Style\")\n",
        "                )\n",
        "\n",
        "if spark.catalog.tableExists('silver_sales.Product'):\n",
        "    dlt_obj = DeltaTable.forName(spark, 'silver_sales.Product')\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.ProductID = src.ProductID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "else:\n",
        "    df.write.format('delta')\\\n",
        "            .mode('append')\\\n",
        "            .option('path', 'abfss://silver@dlcontoso.dfs.core.windows.net/production/Product')\\\n",
        "            .saveAsTable('silver_sales.Product')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/production/ProductSubcategory')\n",
        "\n",
        "df = df.drop(col(\"rowguid\"))\n",
        "\n",
        "\n",
        "if spark.catalog.tableExists('silver_sales.ProductSubcategory'):\n",
        "    dlt_obj = DeltaTable.forName(spark, 'silver_sales.ProductSubcategory')\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.ProductSubcategoryID = src.ProductSubcategoryID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "else:\n",
        "    df.write.format('delta')\\\n",
        "            .mode('append')\\\n",
        "            .option('path', 'abfss://silver@dlcontoso.dfs.core.windows.net/production/ProductSubcategory')\\\n",
        "            .saveAsTable('silver_sales.ProductSubcategory')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.format('parquet')\\\n",
        "            .load('abfss://bronze@dlcontoso.dfs.core.windows.net/production/ProductCategory')\n",
        "\n",
        "df = df.drop(col(\"rowguid\"))\n",
        "\n",
        "if spark.catalog.tableExists('silver_sales.ProductCategory'):\n",
        "    dlt_obj = DeltaTable.forName(spark, 'silver_sales.ProductCategory')\n",
        "    dlt_obj.alias(\"trg\").merge(df.alias(\"src\"), \"trg.ProductCategoryID = src.ProductCategoryID\")\\\n",
        "                        .whenMatchedUpdateAll(condition=\"src.ModifiedDate > trg.ModifiedDate\")\\\n",
        "                        .whenNotMatchedInsertAll()\\\n",
        "                        .execute()\n",
        "else:\n",
        "    df.write.format('delta')\\\n",
        "            .mode('append')\\\n",
        "            .option('path', 'abfss://silver@dlcontoso.dfs.core.windows.net/production/ProductCategory')\\\n",
        "            .saveAsTable('silver_sales.ProductCategory')"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}